---
CKAè€ƒè¯•é¢˜ç›®ç±»å‹  ä¹¦ç­¾ï¼š [K8S](https://kubernetes.io/docs/home/)
---
#### 1. Set configuration context $ kubectl config use-context k8s Monitor the logs of Pod foobar and Extract log lines corresponding to error file-not-found Write them to /opt/KULM00201/foobar
   ```shell
     kubectl logs foobar | grep file-not-found > /logs
   ```
---
#### 2. List all PVs sorted by name saving the full kubectl output to /opt/KUCC0010/my_volumes . Use kubectlâ€™s own functionally for sorting the output, and do not manipulate it any further.
   ```shell
     kubectl get pv --all-namespace --sort-by=.metadata.name > /opt/
   ```
---
#### 3. Ensure a single instance of Pod nginx is running on each node of the kubernetes cluster where nginx also represents the image name which has to be used. Do no override any taints currently in place.
#####   Use Daemonsets to complete this task and use ds.kusc00201 as Daemonset name
   éœ€è¦åˆ é™¤ä¸éœ€è¦çš„éƒ¨åˆ†tolerationså’Œvolume
```yaml
  apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    name: fluentd-elasticsearch
    namespace: kube-system
    labels:
      k8s-app: fluentd-logging
  spec:
    selector:
      matchLabels:
        name: fluentd-elasticsearch
    template:
      metadata:
        labels:
          name: fluentd-elasticsearch
      spec:
  #      tolerations:
        # this toleration is to have the daemonset runnable on master nodes
        # remove it if your masters can't run pods
  #      - key: node-role.kubernetes.io/master
  #        operator: Exists
  #        effect: NoSchedule
        containers:
        - name: fluentd-elasticsearch
  #       image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
          image: nginx
  #        resources:
  #          limits:
  #            memory: 200Mi
  #          requests:
  #            cpu: 100m
  #            memory: 200Mi

```
---
#### 4. Add an init container to lumpy--koala (Which has been defined in spec file /opt/kucc00100/pod-spec-KUCC00100.yaml)The init container should create an empty file named /workdir/calm.txt
####   If /workdir/calm.txt is not detected, the Pod should exit
####   Once the spec file has been updated with the init container definition, the Pod should be created.

---
#### 5.  Create a pod named kucc4 with a single container for each of the following images running inside (there may be between 1 and 4 images specified): nginx + redis + memcached + consul
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kucc
spec:
  containers:
  - name: nginx
    image: nginx
  - name: redis
    image: redis
  - name: memcached
    image: memcached
  - name: consul
    image: consul
```
---
#### 6.  Schedule a Pod as follows:
*    Name: nginx-kusc00101
*    Nmage: nginx
*    Node selector: disk=ssd
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd  
```
---
#### 7.  Create a deployment as follows:
   * Name: nginx-app
   * Using container nginx with version 1.10.2-alpine
   * the deployment should contain 3 replicas
   * Next, deploy the app with new version 1.13.0-alpine by performing a rolling update and record that update.
   * Finally, rollback that update to the previous version 1.10.2-alpine
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.11.9
```
```
kubectl set image deployment nginx-app nginx-app=nginx:1.12.0 --record  (nginx-app containeråå­—)
kubectl rollout history deployment nginx-app
kubectl rollout undo deployment nginx-app    ///æ–°ç‰ˆæœ¬å¥½åƒæ²¡æœ‰è¿™ä¸ªå‘½ä»¤ï¼Œéœ€è¦ç¡®è®¤ä¸‹è€ƒè¯•ç‰ˆæœ¬
```
---
#### 8.  Create and configure the service front-end-service so itâ€™s accessible through NodePort and routes to the existing pod named front-end
```sh
kubectl expose pod fron-end --name=front-end-service --port=80  --type=NodePort     //éœ€è¦å…ˆåˆ›å»ºpodæˆ‘ä¸çŸ¥é“è€ƒè¯•éœ€è¦è‡ªå·±åˆ›å»ºä¸ï¼Œkubectl run fron-end --image=nginx
```
---
#### 9.   Create a Pod as follows:
*	Name: jenkins
*	Using image: jenkins
*	In a new Kubenetes namespace named website-frontend 
```
kubectl create ns website-frontend
kubectl run Jenkins --image=jenkins --namespace=website-frontend     //pod nameä¸èƒ½åŒ…å«å¤§å†™ï¼Œä¸çŸ¥é“æ˜¯ä¸æ˜¯é¢˜ç›®çš„é—®é¢˜
```
---
#### 10. Create a deployment spec file that will:
- #### Launch 7 replicas of the redis image with the label: app_env_stage=dev

- #### Deployment name: kual0020

- #### Save a copy of this spec file to /opt/KUAL00201/deploy_spec.yaml (or .json)

- #### When you are done, clean up (delete) any new k8s API objects that you produced during this task 

```yaml
kubectl run kual00201 --image=redis --labels=app_enb_stage=dev --dry-run=client -oyaml > /opt/KUAL00201/deploy_spec.yaml 
root@k8s-master:~# cat /opt/KUAL00201/deploy_spec.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    app_enb_stage: dev
  name: kual00201
spec:
  containers:
  - image: redis
    name: kual00201
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
root@k8s-master:~# 
```
---
#### 11. Create a file /opt/KUCC00302/kucc00302.txt that lists all pods that implement Service foo in Namespace production.
#### The format of the file should be one pod name per line
```
kubectl get svc -n production --show-labels | grep foo

kubectl get pods -l app=foo(labelæ ‡ç­¾)  | grep -v NAME | awk '{print $1}' >> /opt/KUCC00302/kucc00302.txt
```
---
#### 12. Create a Kubernetes Secret as follows:
- #### Name: super-secret

- #### Credential: alice  or username:bob 

- #### Create a Pod named pod-secrets-via-file using the redis image which mounts a secret named super-secret at /secrets

- #### Create a second Pod named pod-secrets-via-env using the redis image, which exports credential as TOPSECRET


* secrets
```yaml
root@k8s-master:~# echo 'bob' | base64
Ym9iCg==

apiVersion: v1
kind: Secret
metadata:
  name: super-secret
type: Opaque
data:
  username: Ym9iCg==
```
* pod
```
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: "/secret"
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: super-secret
```
* envpod
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod2
spec:
  containers:
  - name: mycontainer
    image: nginx
    env:
      - name: ABC
        valueFrom:
          secretKeyRef:
            name: super-secret
            key: username
  restartPolicy: Never
```
* //éªŒè¯ä¸€ä¸‹ï¼Œè€ƒè¯•ä¸ç”¨
```sh
root@k8s-master:~# kubectl exec -it pod1 -- bash
root@pod1:/data# ls
root@pod1:/data# cat /secret/username 
bob
root@pod1:/data# exit
exit
root@k8s-master:~# kubectl exec -it pod2 -- bash 
root@pod2:/# env | grep ABC
ABC=bob
root@pod2:/# 
```
---
13. Create a pod as follows:
	Name: non-persistent-redis
	Container image: redis
	Named-volume with name: cache-control
	Mount path: /data/redis
	It should launch in the pre-prod namespace and the volume MUST NOT be persistent.
```yaml
kubectl create ns pre-prod

apiVersion: v1
kind: Pod
metadata:
  name: non-presistent-redis
  namespace: pre-prod
spec:
  containers:
  - image: redis
    name: redis
    volumeMounts:
    - mountPath: /data/redis
      name: cache-control
  volumes:
  - name: cache-control
    emptyDir: {}
```
---
#### 14. deploy scale
```sh
kubectl scale deployment nginx-app --replicas=6

```
---
#### 15. Check to see how many nodes are ready (not including nodes tainted NoSchedule) and write the number to /opt/nodenum
```sh
kubectl get node | grep -w  Ready | wc -l     //ready ä¸ªæ•°
kubectl describe nodes | grep Taints | grep -i noschedule | wc -l     //noscheduleä¸ªæ•° 
```
---
#### 16. From the Pod label name=cpu-utilizer, find pods running high CPU workloads and write the name of the Pod consuming most CPU to the file /opt/cpu.txt (which already exists)
```sh
kubectl top pod --sort-by=cpu --namespace kube-system  
```
---
#### 17.  Create a deployment as follows
- #### Name: nginx-dns

- #### Exposed via a service: nginx-dns

- #### Ensure that the service & pod are accessible via their respective DNS records

- #### The container(s) within any Pod(s) running as a part of this deployment should use the nginx image

- #### Next, use the utility nslookup to look up the DNS records of the service & pod and write the output to /opt/service.dns and /opt/pod.dns respectively.

- #### Ensure you use the busybox:1.28 image(or earlier) for any testing, an the latest release has an unpstream bug which impacts thd use of nslookup.

```sh
kubectl create deployment nginx-dns --image=nginx
kubectl expose deployment nginx-dns --port=80 --type=NodePort
root@k8s-master:~# kubectl run busybox -it --rm --image=busybox:1.28 -- sh 
If you don't see a command prompt, try pressing enter.
/ # nslookup nginx-dns
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      nginx-dns
Address 1: 10.107.102.138 nginx-dns.default.svc.cluster.local
/ # 
/ # 
```
---
#### 18. Create a snapshot of the etcd instance running at https://127.0.0.1:2379 saving the snapshot to the file path /data/backup/etcd-snapshot.db
- #### The etcd instance is running etcd version 3.1.10

- #### The following TLS certificates/key are supplied for connecting to the server with etcdctl

- #### CA certificate: /opt/KUCM00302/ca.crt

- #### Client certificate: /opt/KUCM00302/etcd-client.crt

- #### Clientkey:/opt/KUCM00302/etcd-client.key 
```sh
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379  --cacert=ca.pem --cert=server.pem --key=server-key.pem  snapshot save /data/backup/etcd-snapshot.db
```
---
#### 19. Set the node labelled with name=ek8s-node-1 as unavailable and reschedule all the pods running on it.
```
kubectl get nodes -l name=ek8s-node-1
kubectl drain wk8s-node-1  
#å½“ç›®æ ‡nodeä¸Šæœ‰daementSetæ—¶éœ€è¦åŠ ä»¥ä¸‹å‚æ•°ï¼Œæ‰€ä»¥å»ºè®®åŠ ä¸Š
#--ignore-daemonsets=true --delete-local-data=true --force=true
kubectl uncordon k8s-node0*    //æ¢å¤å‘½ä»¤ï¼Œè€ƒè¯•ä¸ç”¨
```
---
#### 20. A Kubernetes worker node, labelled with name=wk8s-node-0 is in state NotReady . Investigate why this is the case, and perform any appropriate steps to bring the node to a Ready state, ensuring that any changes are made permanent.
```
kubectl get nodes | grep NotReady
ssh node  
systemctl status kubelet
systemctl start kubelet   
systemctl enable kubelet
```
---
#### 21. Configure the kubelet systemd managed service, on the node labelled with name=wk8s-node-1, to launch a Pod containing a single container of image nginx named myservice automatically. Any spec files required should be placed in the /etc/kubernetes/manifests directory on the node.
> [å‚è€ƒblog](https://github.com/qiangwum/blogs/blob/main/%E9%9D%99%E6%80%81pod%E7%9A%84%E5%88%9B%E5%BB%BA.md)
---
#### 22.  Determine the node, the failing service and take actions to bring up the failed service and restore the health of the cluster. Ensure that any changes are made permanently.
#### The worker node in this cluster is labelled with name=bk8s-node-0
```
æƒ…å½¢ä¸€ï¼škubectl å‘½ä»¤èƒ½ç”¨ 
kubectl get cs å¥åº·æ£€æŸ¥  çœ‹manager-controller  æ˜¯å¦ready   
å¦‚æœä¸ready   systemctl start kube-manager-controller.service   
æƒ…å½¢äºŒï¼škubectl å‘½ä»¤ä¸èƒ½ç”¨
2ï¼Œsshç™»é™†åˆ°bk8 -master-0ä¸Šæ£€æŸ¥æœåŠ¡ï¼Œå¦‚masterä¸Šçš„4å¤§æœåŠ¡ï¼Œ
api-server/schedule/controllor-manager/etcd
systemctl list-utils-files | grep controller-manager    æ²¡æœ‰æœåŠ¡
systemctl list-utils-files | grep api-server       æ²¡æœ‰æœåŠ¡
3,æ­¤åˆ»è¿›å…¥/etc/kubernetes/manifest  æ–‡ä»¶å¤¹ä¸­ï¼Œå¯ä»¥çœ‹åˆ°api-server.yaml  controller-manager.yamlç­‰4ä¸ªæ–‡ä»¶ï¼Œè¯´æ˜è¿™å‡ ä¸ªæœåŠ¡æ˜¯ä»¥podæ–¹å¼æä¾›æœåŠ¡çš„ã€‚
4, systemctl status kubelet     çœ‹åˆ°æ˜¯æ­£å¸¸å¯åŠ¨çš„ï¼Œ
è¯´æ˜api-server   controlloer-manager    etcd    schedule  è¿™å‡ ä¸ªpod æ²¡å¯åŠ¨ï¼Œ
æ£€æŸ¥é™æ€podé…ç½®,åœ¨/var/lib/systemd/system/kubelet.service è¿™ä¸ªæ–‡ä»¶é‡Œæ£€æŸ¥é…ç½®çœ‹åˆ°é™æ€é…ç½®è·¯å¾„é”™è¯¯
è€ƒè¯•ç¯å¢ƒæŠŠæ­£ç¡®çš„/etc/kubernetes/manifest  æ¢æˆäº†/etc/kubernetes/DODKSIYF è·¯å¾„ï¼Œæ­¤è·¯å¾„å¹¶ä¸å­˜åœ¨ï¼ŒæŠŠè¿™ä¸ªé”™è¯¯çš„è·¯å¾„æ¢æˆåˆ°å­˜æ”¾api/controller-manager/etcd/scheduleè¿™å‡ ä¸ªyamlæ–‡ä»¶å­˜æ”¾çš„è·¯å¾„ï¼Œé‡å¯Kubeletï¼Œæ’é”™å®Œæˆã€‚
å†æŸ¥çœ‹nodeå•¥çš„ï¼Œå°±OKäº†
```
---
#### 23. Creae a persistent volume with name app-config of capacity 1Gi and access mode ReadWriteOnce. The type of volume is hostPath and its location is /srv/app-config

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: app-config
spec
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  hostPath:
    path: /srv/app-config
```
---
#### 24. ç¯å¢ƒæ­å»º
* å¯ä»¥å‚è€ƒå¸–å­[k8så®‰è£…](https://github.com/qiangwum/blogs/blob/main/ubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AE%89%E8%A3%85k8s.md)
```sh
# masterå’Œnodeä¸Š:å®‰è£…kubeam kubelet kubectl

cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system

sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

#masteråˆå§‹åŒ–
kubeadm init   --ignore-preflight-errors=xxx

#masterå®‰è£…ç½‘ç»œ
kubectl apply -f https://docs.projectcalico.org/v3.11/manifests/calico.yaml

#nodeåŠ å…¥é›†ç¾¤
```
---
#### 24. TLSé—®é¢˜ ï¼ˆä¸€é“å¾ˆé•¿çš„é¢˜ç›®ï¼Œå»ºè®®æ”¾å¼ƒï¼Œéš¾åº¦ç‰¹åˆ«å¤§ï¼‰

> [yamlå‚è€ƒ](https://github.com/qiangwum/script/blob/main/pod.yaml)
> ğŸ˜


